{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f753343",
   "metadata": {},
   "source": [
    "Необходимо обучить модель для предсказания цены машины. База данных основана на объявленияъ с сайта \"Юла\".\n",
    "\n",
    "Оцените качество работы созданной сети, определив средний процент ошибки на проверочной выборке. (Для этого потребуется привести предсказанные моделью значения к первоначальному диапазону цен. Это можно сделать с помощью следующего метода: predict_inverse = y_scaler.inverse_transfrom(predict).flatten() где predict - результат предсказания\n",
    "модели). Затем подсчитайте ошибку на каждом примере тестовой выборки и суммарный процент ошибки. \n",
    "\n",
    "Рекомендации:\n",
    "- В качестве ошибки рекомендуется использовать среднеквадратическую ошибку (mse).\n",
    "- Метрику для данной задачи можно не использовать.\n",
    "- Последний слой модели должен иметь 1 нейрон."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3ab8a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# отрисовка изображений в ноутбуке, а не в файл или консоль\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fc6b096",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential  # НС прямого распространения\n",
    "from keras.layers import Dense, Activation, Dropout, BatchNormalization  # основные слои\n",
    "from keras import utils  # утилиты для to_categorical\n",
    "from keras.preprocessing import image  # Для отрисовки изображения\n",
    "from keras.optimizers import Adadelta, Adam  # Алгоритмы оптимизации для настройки скорости обучения\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler  # Функции для нормализации данных\n",
    "from sklearn import preprocessing  # Пакет для предварительной обработки данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9469a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv(r\"\\Neural networks\\Data\\cars_new.csv\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14ba2c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = cars.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c412a552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mark</th>\n",
       "      <th>model</th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>mileage</th>\n",
       "      <th>body</th>\n",
       "      <th>kpp</th>\n",
       "      <th>fuel</th>\n",
       "      <th>volume</th>\n",
       "      <th>power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kia</td>\n",
       "      <td>cerato</td>\n",
       "      <td>996000</td>\n",
       "      <td>2018</td>\n",
       "      <td>28000</td>\n",
       "      <td>седан</td>\n",
       "      <td>автомат</td>\n",
       "      <td>бензин</td>\n",
       "      <td>2.0</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daewoo</td>\n",
       "      <td>nexia 1 поколение [2-й рестайлинг]</td>\n",
       "      <td>140200</td>\n",
       "      <td>2012</td>\n",
       "      <td>60500</td>\n",
       "      <td>седан</td>\n",
       "      <td>механика</td>\n",
       "      <td>бензин</td>\n",
       "      <td>1.5</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>suzuki</td>\n",
       "      <td>jimny 3 поколение [рестайлинг]</td>\n",
       "      <td>750000</td>\n",
       "      <td>2011</td>\n",
       "      <td>29000</td>\n",
       "      <td>внедорожник</td>\n",
       "      <td>автомат</td>\n",
       "      <td>бензин</td>\n",
       "      <td>1.3</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mark                               model   price  year  mileage  \\\n",
       "0     kia                              cerato  996000  2018    28000   \n",
       "1  daewoo  nexia 1 поколение [2-й рестайлинг]  140200  2012    60500   \n",
       "2  suzuki      jimny 3 поколение [рестайлинг]  750000  2011    29000   \n",
       "\n",
       "          body       kpp    fuel  volume  power  \n",
       "0        седан   автомат  бензин     2.0  150.0  \n",
       "1        седан  механика  бензин     1.5   80.0  \n",
       "2  внедорожник   автомат  бензин     1.3   85.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4e41e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70112, 10)\n"
     ]
    }
   ],
   "source": [
    "print(cars.values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b22e8b5",
   "metadata": {},
   "source": [
    "Напишем функцмю для преобразования столбца данных pandas DataFrame в OneHot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afd5eb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelsToOneHot(column):\n",
    "    vocab, indexes = np.unique(cars[column], return_inverse=True)  # Получаем массив уникальных значений для солбца и тотже столбец в виде индексов\n",
    "    oneHotData = utils.to_categorical(indexes, num_classes=len(vocab))  # Преобразуем индексы в onehot\n",
    "    return oneHotData, vocab.tolist()  # Возвращаем индексы в OneHot и список уникальных значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9389bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Укажем столбцы с категориальными данными\n",
    "labelData = ['mark', 'model', 'body', 'kpp', 'fuel']\n",
    "vocabulary = []  # Список для списков всех уникальных значений. Его можно использовать для обратного преобразования из onehot в категориальные данные\n",
    "oneHot = []  # Список для массивов onehot для всех столбцов\n",
    "for column in labelData:\n",
    "    oneHotData, vocab = labelsToOneHot(column)  # получим индексы и список уникальных значений для каждого столбца\n",
    "    vocabulary.append(vocab)\n",
    "    oneHot.append(oneHotData)\n",
    "    oneHotArray = np.concatenate([i for i in oneHot], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a765a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "oneHotArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e4d4de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70112, 3202)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneHotArray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9421518d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число уникальных элементов в mark : 21\n",
      "Число уникальных элементов в model : 3156\n",
      "Число уникальных элементов в body : 16\n",
      "Число уникальных элементов в kpp : 4\n",
      "Число уникальных элементов в fuel : 5\n"
     ]
    }
   ],
   "source": [
    "for voc in vocabulary:\n",
    "    print('Число уникальных элементов в', labelData[vocabulary.index(voc)], ':', len(voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a68037",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocabulary[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "266754fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посчитаем количество всех автомобилей\n",
    "lenmarks = len(vocabulary[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e17e57",
   "metadata": {},
   "source": [
    "Получим из какой-нибудь строчки массива подмассив, принадлежащий к марке (порядок строчек сообтветствует порядку в DataFrame). Затем переведем его в индекс класса и сверим его с индексом в vocabulary (убедимся, что они идентичны)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d41e9a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mercedes-benz'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(oneHotArray[100, :lenmarks])\n",
    "vocabulary[0][np.argmax(oneHotArray[100, :lenmarks])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07e30e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определим и запомним целевую переменную - цены\n",
    "prices = np.array(cars['price'], dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b884fffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запомним числовые параметры и нормализуем\n",
    "years = preprocessing.scale(cars['year'])\n",
    "mileages = preprocessing.scale(cars['mileage'])\n",
    "volumes = preprocessing.scale(cars['volume'])\n",
    "powers = preprocessing.scale(cars['power'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "558a24bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.0392214499056326e-14)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e18972c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mileages.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "576ba1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.22902799 -0.95629921 -0.87163298 ...  0.22902799 -0.75310026\n",
      "  2.34568371]\n"
     ]
    }
   ],
   "source": [
    "print(powers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2991a651",
   "metadata": {},
   "source": [
    "Создадим обучающую выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4ccc64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for _id, car in enumerate(np.array(cars)):\n",
    "    # добавляем цену в целевую переменную\n",
    "    y_train.append(prices[_id])\n",
    "\n",
    "    # В x_train добавляем все параметры. Категориальные переменные добавляем в виде ohe, числовые - как есть\n",
    "    x_tr = oneHotArray[_id]\n",
    "    x_tr = np.append(x_tr, years[_id])\n",
    "    x_tr = np.append(x_tr, mileages[_id])\n",
    "    x_tr = np.append(x_tr, volumes[_id])\n",
    "    x_tr = np.append(x_tr, powers[_id])\n",
    "    \n",
    "    # добавим текущую строку в общий x_train\n",
    "    x_train.append(x_tr)\n",
    "\n",
    "# Превращаем лист в np.array после завершения цикла\n",
    "x_train = np.array(x_train, dtype=np.float64)\n",
    "y_train = np.array(y_train, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d798d98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70112, 3206)\n",
      "(70112,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26c044e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[ 0.          1.          0.          0.          0.          0.\n",
      "  0.          1.          0.          0.          0.          1.\n",
      "  0.          0.          0.          0.          1.52001176 -1.4002026\n",
      "  0.12284181  0.22902799]\n"
     ]
    }
   ],
   "source": [
    "# Выведем один x_train\n",
    "print(x_train[0, :20])\n",
    "print(x_train[0,-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d1f1f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 996000.  140200.  750000.  970000.  205000.  985000.  589000.  500000.\n",
      " 1320000.  270000.]\n"
     ]
    }
   ],
   "source": [
    "# Выведем один y_train\n",
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a946da",
   "metadata": {},
   "source": [
    "Разделяем данные на тренировочную и тестовую выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7dd0a56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9e42c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74ba1ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scaler = StandardScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea493136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63100,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66c613e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 430000.  950000.  130000.  455000. 1450000.]\n",
      "[-0.15641677  0.67288704 -0.63486128 -0.1165464   1.47029455]\n"
     ]
    }
   ],
   "source": [
    "# Выведем базовый и нормированный y_train\n",
    "print(y_train[:5])\n",
    "print(y_train_scaled[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6dd4af8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63100, 3206)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b885d7",
   "metadata": {},
   "source": [
    "Построим модель нейронной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13028c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ula = Sequential()\n",
    "model_ula.add(Dense(1000, activation='relu', input_shape=(x_train.shape[1],)))\n",
    "model_ula.add(Dropout(0.2))\n",
    "model_ula.add(Dense(100, activation='relu'))\n",
    "model_ula.add(Dense(1, activation='linear'))\n",
    "\n",
    "model_ula.compile(optimizer=Adam(learning_rate=0.0001), loss='mse')\n",
    "\n",
    "# В обучающей выборке будет 50000 примеров\n",
    "n_val = 50000\n",
    "history = model_ula.fit(x_train[:n_val], y_train_scaled[:n_val],\n",
    "                        batch_size=32,\n",
    "                        epochs=15,\n",
    "                        validation_data=(x_train[n_val:], y_train_scaled[n_val:]),\n",
    "                        verbose=1)\n",
    "# Отобразим графики ошибки обучения по обучающей выборке и по проверочной выборки на всех эпохах\n",
    "plt.plot(history.history['loss'], label='Ошибка на обучающей выборке')\n",
    "plt.plot(history.history['val_loss'], label='Ошибка на проверочной выборке')\n",
    "plt.xlabel('Эпоха обучения')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c5c80fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m220/220\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Средняя ошибка:  67452\n",
      "Средняя цена:  524536\n",
      "Сумарный процент ошибки :13%\n"
     ]
    }
   ],
   "source": [
    "# предсказываем проверочную выборку\n",
    "prediction = model_ula.predict(x_test)\n",
    "\n",
    "# Меняем масштаб обратно от нормированного к оригинальному\n",
    "prediction = y_scaler.inverse_transform(prediction).flatten()\n",
    "\n",
    "# Посчитаем среднюю цену, среднюю ошибку и средний процент ошибки\n",
    "mean_delta = np.mean(abs(prediction - y_test))\n",
    "mean_price = np.mean(y_test)\n",
    "\n",
    "print('Средняя ошибка: ', round(mean_delta) )\n",
    "print('Средняя цена: ', round(mean_price))\n",
    "print('Сумарный процент ошибки :', round(100*mean_delta/mean_price), '%', sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0322a9d7",
   "metadata": {},
   "source": [
    "**Протестируем модель**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "50b8a091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Искусственные данные для тестирования\n",
    "test_data = [\n",
    "    ['kia', 'cerato', 2019, 25000, 'седан', 'автомат', 'бензин', 2.0, 150.0],\n",
    "    ['daewoo', 'nexia', 2010, 120000, 'седан', 'механика', 'бензин', 1.5, 80.0],\n",
    "    ['suzuki', 'jimny', 2020, 10000, 'внедорожник', 'автомат', 'бензин', 1.3, 85.0],\n",
    "    ['bmw', 'x5', 2015, 80000, 'внедорожник', 'автомат', 'дизель', 3.0, 250.0]\n",
    "]\n",
    "\n",
    "# Создаем DataFrame\n",
    "columns = ['mark', 'model', 'year', 'mileage', 'body', 'kpp', 'fuel', 'volume', 'power']\n",
    "test_df = pd.DataFrame(test_data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "924ffb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для преобразования категориальных признаков в one-hot encoding\n",
    "def labelsToOneHot(column, vocabulary):\n",
    "    # Создаем словарь для соответствия значений индексам\n",
    "    value_to_index = {v: i for i, v in enumerate(vocabulary)}\n",
    "    indexes = np.array([value_to_index.get(val, 0) for val in column])  # 0 для неизвестных значений\n",
    "    oneHotData = utils.to_categorical(indexes, num_classes=len(vocabulary))\n",
    "    return oneHotData\n",
    "\n",
    "# Преобразуем категориальные признаки (используем сохраненные vocabulary)\n",
    "labelData = ['mark', 'model', 'body', 'kpp', 'fuel']\n",
    "oneHot_test = []\n",
    "\n",
    "for i, column in enumerate(labelData):\n",
    "    oneHotData = labelsToOneHot(test_df[column], vocabulary[i])\n",
    "    oneHot_test.append(oneHotData)\n",
    "\n",
    "# Объединяем one-hot encoded признаки\n",
    "oneHotArray_test = np.concatenate([i for i in oneHot_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8169976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Сохраняем scaler'ы для числовых признаков (делается один раз при обучении)\n",
    "year_scaler = StandardScaler()\n",
    "mileage_scaler = StandardScaler()\n",
    "volume_scaler = StandardScaler()\n",
    "power_scaler = StandardScaler()\n",
    "\n",
    "# При обучении:\n",
    "year_scaler.fit(cars[['year']])  # Обучаем scaler на тренировочных данных\n",
    "mileage_scaler.fit(cars[['mileage']])\n",
    "volume_scaler.fit(cars[['volume']])\n",
    "power_scaler.fit(cars[['power']])\n",
    "\n",
    "# Нормализуем числовые признаки тестовых данных\n",
    "years_test = year_scaler.transform(test_df[['year']])\n",
    "mileages_test = mileage_scaler.transform(test_df[['mileage']])\n",
    "volumes_test = volume_scaler.transform(test_df[['volume']])\n",
    "powers_test = power_scaler.transform(test_df[['power']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5f870a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединяем все признаки\n",
    "x_test_processed = np.column_stack([\n",
    "    oneHotArray_test,\n",
    "    years_test,\n",
    "    mileages_test,\n",
    "    volumes_test,\n",
    "    powers_test\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f019f98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Автомобиль 1:\n",
      "  Марка: kia, Модель: cerato, Год: 2019, Пробег: 25000\n",
      "  Тип кузова: седан, КПП: автомат, Топливо: бензин\n",
      "  Объем: 2.0, Мощность: 150.0\n",
      "  Предсказанная цена: 1147663.38 руб.\n",
      "\n",
      "Автомобиль 2:\n",
      "  Марка: daewoo, Модель: nexia, Год: 2010, Пробег: 120000\n",
      "  Тип кузова: седан, КПП: механика, Топливо: бензин\n",
      "  Объем: 1.5, Мощность: 80.0\n",
      "  Предсказанная цена: 119326.16 руб.\n",
      "\n",
      "Автомобиль 3:\n",
      "  Марка: suzuki, Модель: jimny, Год: 2020, Пробег: 10000\n",
      "  Тип кузова: внедорожник, КПП: автомат, Топливо: бензин\n",
      "  Объем: 1.3, Мощность: 85.0\n",
      "  Предсказанная цена: 1419066.25 руб.\n",
      "\n",
      "Автомобиль 4:\n",
      "  Марка: bmw, Модель: x5, Год: 2015, Пробег: 80000\n",
      "  Тип кузова: внедорожник, КПП: автомат, Топливо: дизель\n",
      "  Объем: 3.0, Мощность: 250.0\n",
      "  Предсказанная цена: 3885555.50 руб.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Предсказание\n",
    "predictions_scaled = model_ula.predict(x_test_processed)\n",
    "\n",
    "# Обратное преобразование масштабированной целевой переменной\n",
    "predictions = y_scaler.inverse_transform(predictions_scaled)\n",
    "\n",
    "# Вывод результатов\n",
    "for i, (data, pred) in enumerate(zip(test_data, predictions)):\n",
    "    print(f\"Автомобиль {i+1}:\")\n",
    "    print(f\"  Марка: {data[0]}, Модель: {data[1]}, Год: {data[2]}, Пробег: {data[3]}\")\n",
    "    print(f\"  Тип кузова: {data[4]}, КПП: {data[5]}, Топливо: {data[6]}\")\n",
    "    print(f\"  Объем: {data[7]}, Мощность: {data[8]}\")\n",
    "    print(f\"  Предсказанная цена: {pred[0]:.2f} руб.\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9c623c",
   "metadata": {},
   "source": [
    "# Сохранение модели и вспомогательных объектов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2710e042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "from keras.models import save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29d0a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем модель\n",
    "save_model(model_ula, 'car_price_model.h5')\n",
    "\n",
    "# Сохраняем scaler'ы для числовых признаков\n",
    "joblib.dump(year_scaler, 'scalers/year_scaler.pkl')\n",
    "joblib.dump(mileage_scaler, 'scalers/mileage_scaler.pkl')\n",
    "joblib.dump(volume_scaler, 'scalers/volume_scaler.pkl')\n",
    "joblib.dump(power_scaler, 'scalers/power_scaler.pkl')\n",
    "\n",
    "# Сохраняем scaler для целевой переменной\n",
    "joblib.dump(y_scaler, 'scalers/y_scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f655650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем vocabulary для категориальных признаков\n",
    "vocabulary_dict = {\n",
    "    'mark': vocabulary[0],\n",
    "    'model': vocabulary[1],\n",
    "    'body': vocabulary[2],\n",
    "    'kpp': vocabulary[3],\n",
    "    'fuel': vocabulary[4]\n",
    "}\n",
    "joblib.dump(vocabulary_dict, 'vocabulary.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d374dc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем порядок признаков (важно для сборки)\n",
    "feature_order = [\n",
    "    *[f'mark_{v}' for v in vocabulary[0]],\n",
    "    *[f'model_{v}' for v in vocabulary[1]],\n",
    "    *[f'body_{v}' for v in vocabulary[2]],\n",
    "    *[f'kpp_{v}' for v in vocabulary[3]],\n",
    "    *[f'fuel_{v}' for v in vocabulary[4]],\n",
    "    'year', 'mileage', 'volume', 'power'\n",
    "]\n",
    "joblib.dump(feature_order, 'feature_order.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75533993",
   "metadata": {},
   "source": [
    "# Загрузка и использование для предсказаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f13bed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "from keras import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69548100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем все необходимые объекты\n",
    "model = load_model('car_price_model.h5')\n",
    "year_scaler = joblib.load('scalers/year_scaler.pkl')\n",
    "mileage_scaler = joblib.load('scalers/mileage_scaler.pkl')\n",
    "volume_scaler = joblib.load('scalers/volume_scaler.pkl')\n",
    "power_scaler = joblib.load('scalers/power_scaler.pkl')\n",
    "y_scaler = joblib.load('scalers/y_scaler.pkl')\n",
    "vocabulary = joblib.load('vocabulary.pkl')\n",
    "feature_order = joblib.load('feature_order.pkl')\n",
    "\n",
    "def preprocess_input(new_data):\n",
    "    \"\"\"Функция для предобработки новых данных\"\"\"\n",
    "    # Преобразуем в DataFrame если это еще не сделано\n",
    "    if not isinstance(new_data, pd.DataFrame):\n",
    "        new_df = pd.DataFrame([new_data], columns=['mark', 'model', 'year', 'mileage', \n",
    "                                                'body', 'kpp', 'fuel', 'volume', 'power'])\n",
    "    else:\n",
    "        new_df = new_data.copy()\n",
    "    \n",
    "    # One-Hot Encoding для категориальных признаков\n",
    "    one_hot_encoded = []\n",
    "    for col in ['mark', 'model', 'body', 'kpp', 'fuel']:\n",
    "        vocab = vocabulary[col]\n",
    "        value_to_idx = {v: i for i, v in enumerate(vocab)}\n",
    "        indexes = np.array([value_to_idx.get(val, -1) for val in new_df[col]])\n",
    "        one_hot = utils.to_categorical(indexes, num_classes=len(vocab))\n",
    "        one_hot_encoded.append(one_hot)\n",
    "    \n",
    "    one_hot_array = np.concatenate(one_hot_encoded, axis=1)\n",
    "    \n",
    "    # Масштабирование числовых признаков\n",
    "    years = year_scaler.transform(new_df[['year']])\n",
    "    mileages = mileage_scaler.transform(new_df[['mileage']])\n",
    "    volumes = volume_scaler.transform(new_df[['volume']])\n",
    "    powers = power_scaler.transform(new_df[['power']])\n",
    "    \n",
    "    # Собираем все признаки вместе\n",
    "    processed = np.column_stack([\n",
    "        one_hot_array,\n",
    "        years,\n",
    "        mileages,\n",
    "        volumes,\n",
    "        powers\n",
    "    ])\n",
    "    \n",
    "    return processed\n",
    "\n",
    "def predict_price(car_data):\n",
    "    \"\"\"Функция для предсказания цены\"\"\"\n",
    "    processed_data = preprocess_input(car_data)\n",
    "    scaled_pred = model.predict(processed_data)\n",
    "    prediction = y_scaler.inverse_transform(scaled_pred)\n",
    "    return prediction[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572c71ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример использования\n",
    "new_car = {\n",
    "    'mark': 'kia',\n",
    "    'model': 'cerato',\n",
    "    'year': 2020,\n",
    "    'mileage': 15000,\n",
    "    'body': 'седан',\n",
    "    'kpp': 'автомат',\n",
    "    'fuel': 'бензин',\n",
    "    'volume': 2.0,\n",
    "    'power': 150.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca055d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_price = predict_price(new_car)\n",
    "print(f\"Предсказанная цена: {predicted_price:.2f} руб.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2d44e4",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "**Структура папок проекта**\n",
    "\n",
    "- │   car_price_model.h5 - # Сохраненная модель\n",
    "- │   vocabulary.pkl  -  # Словари для категориальных признаков\n",
    "- │   feature_order.pkl  -  # Порядок признаков\n",
    "- │   predict.py - # Скрипт для предсказаний\n",
    "- │   train.py  -  # Скрипт для обучения\n",
    "- │\n",
    "- └───scalers\n",
    "    - │   year_scaler.pkl\n",
    "    - │   mileage_scaler.pkl\n",
    "    - │   volume_scaler.pkl\n",
    "    - │   power_scaler.pkl\n",
    "    - │   y_scaler.pkl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
